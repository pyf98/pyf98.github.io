---
layout: about
title: About
permalink: /
subtitle: <strong>Research Scientist, NVIDIA</strong>

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: 

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

<!-- ⭐ <span style="color: red; font-weight: bold;">Now seeking full-time positions in speech and language processing</span> ⭐ -->

I am currently a Research Scientist at NVIDIA NeMo Speech AI team, working on multimodal large language models and full-duplex speech-to-speech models. I received my Ph.D. in [Electrical and Computer Engineering](https://www.ece.cmu.edu/) from [Carnegie Mellon University](https://www.cmu.edu/), Pittsburgh, PA, USA in 2025, where I was fortunate to be supervised by Prof. [Shinji Watanabe](https://sites.google.com/view/shinjiwatanabe) (Sep 2021 - May 2025) and Prof. [Ian Lane](https://nlp.ucsc.edu/people/nlp-faculty/ian-lane/) (Aug 2020 - Aug 2021; now at UC, Santa Cruz). Prior to that, I received my bachelor's degree from the [Department of Electronic Engineering](https://www.ee.tsinghua.edu.cn/en/), [Tsinghua University](https://www.tsinghua.edu.cn/en/), Beijing, China in 2020.

In Summer 2024, I was an AI Research Intern at [NVIDIA](https://www.nvidia.com/en-us/) NeMo, where I worked on joint speech-text language models. In Summer 2023, I was a research scientist intern at [Meta AI](https://ai.meta.com/) FAIR and worked on speech language models for voice-preserved textless speech-to-speech translation. In Summer 2022, I worked as a speech recognition intern at [ASAPP](https://www.asapp.com/) about speech model compression.

My research area is speech and language processing. 
My Ph.D. thesis is ["Towards Effective and Efficient Open Speech Foundation Models"](https://kilthub.cmu.edu/articles/thesis/Towards_Effective_and_Efficient_Open_Speech_Foundation_Models/29089808). 
Most of my works have been open sourced in a widely-used speech processing toolkit, [ESPnet](https://github.com/espnet/espnet). 
During my Ph.D. at [CMU WAVLab](https://www.wavlab.org/), I led the [Open Whisper-style Speech Models (OWSM)](https://www.wavlab.org/activities/2024/owsm/) project, developing the first large-scale, fully open speech foundation model from academia. 
Now I am interested in spoken language models.

I published first-authored papers at top-tier AI/speech conferences, such as ICML, ACL, NAACL, ICASSP, and INTERSPEECH. Several projects that I was involved in received notable recognition, including [EMNLP 2024 Best Paper Award](https://2024.emnlp.org/program/best_papers/), IEEE SLT 2024 Best Paper Award, [ICASSP 2023 Top 3% Paper Recognition (3 papers)](https://2023.ieeeicassp.org/top-3-percent-paper-recognitions/), and SPIE Medical Imaging 2020 Best Student Paper Award Finalist.

I have been the primary contributor to several projects:

- Speech encoder architecture design: [Branchformer (ICML'22)](https://proceedings.mlr.press/v162/peng22a.html), [E-Branchformer vs Conformer (INTERSPEECH'23)](https://www.isca-archive.org/interspeech_2023/peng23b_interspeech.pdf)
- Speech model compression: [I3D (ICASSP'23 Top 3%)](https://arxiv.org/abs/2303.07624), [HJ-Pruning (ICASSP'23 Top 3%)](https://arxiv.org/abs/2302.14132), [DPHuBERT (INTERSPEECH'23)](https://www.isca-archive.org/interspeech_2023/peng23c_interspeech.html)
- Open speech foundation models: [OWSM (ASRU'23)](https://arxiv.org/abs/2309.13876), [OWSM v3.1 (INTERSPEECH'24)](https://arxiv.org/abs/2401.16658), [OWSM-CTC (ACL'24)](https://aclanthology.org/2024.acl-long.549/), [OWSM v4 (INTERSPEECH'25)](https://arxiv.org/abs/2506.00338)
- Speech language models: [SpeechLM analysis](https://arxiv.org/abs/2403.12402), [MSLM-S2ST](https://arxiv.org/abs/2403.12408), [VoiceTextBlender (NAACL'25)](https://arxiv.org/abs/2410.17485), [SLM Survey](https://arxiv.org/abs/2504.08528)
