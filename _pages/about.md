---
layout: about
title: About
permalink: /
subtitle: <strong>PhD Candidate, Carnegie Mellon University</strong>

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: 

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

⭐ <span style="color: red; font-weight: bold;">Now seeking full-time positions in speech and language processing (expected to start in Summer 2025)</span> ⭐

I am a final-year Ph.D. student in the [Department of Electrical and Computer Engineering](https://www.ece.cmu.edu/) at [Carnegie Mellon University](https://www.cmu.edu/). I am fortunate to be supervised by Prof. [Shinji Watanabe](https://sites.google.com/view/shinjiwatanabe) (Sep 2021 - now) and Prof. [Ian Lane](https://nlp.ucsc.edu/people/nlp-faculty/ian-lane/) (Aug 2020 - Aug 2021; now at UC, Santa Cruz). I received my bachelor’s degree from the [Department of Electronic Engineering](https://www.ee.tsinghua.edu.cn/en/) at [Tsinghua University](https://www.tsinghua.edu.cn/en/) in 2020.

In Summer 2024, I was an AI Research Intern at [NVIDIA](https://www.nvidia.com/en-us/) NeMo, where I worked on joint speech-text language models. In Summer 2023, I was a research scientist intern at [Meta AI](https://ai.meta.com/) FAIR and worked on speech language models for voice-preserved textless speech-to-speech translation. In Summer 2022, I worked as a speech recognition intern at [ASAPP](https://www.asapp.com/) about speech model compression.

My research area is speech and language processing. My Ph.D. thesis is to develop effective and efficient open speech foundation models. I have led the project of [Open Whisper-style Speech Models (OWSM)](https://www.wavlab.org/activities/2024/owsm/) at [CMU WAVLab](https://www.wavlab.org/), developing the first large-scale, fully open speech foundation model from academia. Recently, I am also interested in integrating speech capabilities into large language models.

Throughout my Ph.D. program, I have published first-authored papers in top-tier ML/NLP/Speech conferences, including ICML, ACL, ICASSP, INTERSPEECH, ASRU, and SLT. I am also a contributor to a widely used speech processing toolkit, [ESPnet](https://github.com/espnet/espnet). Specifically, I have been the primary contributor to several major projects:

- Novel speech encoder architecture: [Branchformer (ICML’22)](https://proceedings.mlr.press/v162/peng22a.html), [E-Branchformer vs Conformer (INTERSPEECH'23)](https://www.isca-archive.org/interspeech_2023/peng23b_interspeech.pdf)
- Speech model compression: [I3D (ICASSP’23 Top 3%)](https://arxiv.org/abs/2303.07624), [HJ-Pruning (ICASSP’23 Top 3%)](https://arxiv.org/abs/2302.14132), [DPHuBERT (INTERSPEECH’23)](https://www.isca-archive.org/interspeech_2023/peng23c_interspeech.html)
- Open speech foundation models: [OWSM (ASRU’23)](https://arxiv.org/abs/2309.13876), [OWSM v3.1 (INTERSPEECH’24)](https://arxiv.org/abs/2401.16658), [OWSM-CTC (ACL’24)](https://aclanthology.org/2024.acl-long.549/)
- Speech language models: [SpeechLM analysis](https://arxiv.org/abs/2403.12402), [MSLM-S2ST](https://arxiv.org/abs/2403.12408), [VoiceTextBlender](https://arxiv.org/abs/2410.17485), and more to follow
