<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Yifan Peng </title> <meta name="author" content="Yifan Peng"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pyf98.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Yifan</span> Peng </h1> <p class="desc"><strong>PhD Candidate, Carnegie Mellon University</strong></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_pic.jpg?5681c53a8ce0897349feac8a2ea17511" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>I am a final-year Ph.D. student in the <a href="https://www.ece.cmu.edu/" rel="external nofollow noopener" target="_blank">Department of Electrical and Computer Engineering</a> at <a href="https://www.cmu.edu/" rel="external nofollow noopener" target="_blank">Carnegie Mellon University</a>. I am fortunate to be supervised by Prof. <a href="https://sites.google.com/view/shinjiwatanabe" rel="external nofollow noopener" target="_blank">Shinji Watanabe</a> (Sep 2021 - now) and Prof. <a href="https://nlp.ucsc.edu/people/nlp-faculty/ian-lane/" rel="external nofollow noopener" target="_blank">Ian Lane</a> (Aug 2020 - Aug 2021; now at UC, Santa Cruz). I received my bachelor’s degree from the <a href="https://www.ee.tsinghua.edu.cn/en/" rel="external nofollow noopener" target="_blank">Department of Electronic Engineering</a> at <a href="https://www.tsinghua.edu.cn/en/" rel="external nofollow noopener" target="_blank">Tsinghua University</a> in 2020.</p> <p>In Summer 2024, I was an AI Research Intern at <a href="https://www.nvidia.com/en-us/" rel="external nofollow noopener" target="_blank">NVIDIA</a> NeMo, where I worked on joint speech-text language models. In Summer 2023, I was a research scientist intern at <a href="https://ai.meta.com/" rel="external nofollow noopener" target="_blank">Meta AI</a> FAIR and worked on speech language models for voice-preserved textless speech-to-speech translation. In Summer 2022, I worked as a speech recognition intern at <a href="https://www.asapp.com/" rel="external nofollow noopener" target="_blank">ASAPP</a> about speech model compression.</p> <p>My research area is speech and language processing. My Ph.D. thesis is to develop effective and efficient open speech foundation models. I have led the project of <a href="https://www.wavlab.org/activities/2024/owsm/" rel="external nofollow noopener" target="_blank">Open Whisper-style Speech Models (OWSM)</a> at <a href="https://www.wavlab.org/" rel="external nofollow noopener" target="_blank">CMU WAVLab</a>, developing the first large-scale, fully open speech foundation model from academia. Recently, I am also interested in integrating speech capabilities into large language models.</p> <p>I published first-authored papers at top-tier AI/speech conferences, such as ICML, ACL, ICASSP, and INTERSPEECH. Several projects received notable recognition, including the Best Paper Award at SLT 2024, Best Paper Award at EMNLP 2024, Top 3% Paper Recognition at ICASSP 2023 (3 papers), and Best Student Paper Award Finalist at SPIE Medical Imaging 2020. I also contribute to a widely used speech processing toolkit, <a href="https://github.com/espnet/espnet" rel="external nofollow noopener" target="_blank">ESPnet</a>. Specifically, I have been the primary contributor to several major projects:</p> <ul> <li>Novel speech encoder architecture: <a href="https://proceedings.mlr.press/v162/peng22a.html" rel="external nofollow noopener" target="_blank">Branchformer (ICML’22)</a>, <a href="https://www.isca-archive.org/interspeech_2023/peng23b_interspeech.pdf" rel="external nofollow noopener" target="_blank">E-Branchformer vs Conformer (INTERSPEECH’23)</a> </li> <li>Speech model compression: <a href="https://arxiv.org/abs/2303.07624" rel="external nofollow noopener" target="_blank">I3D (ICASSP’23 Top 3%)</a>, <a href="https://arxiv.org/abs/2302.14132" rel="external nofollow noopener" target="_blank">HJ-Pruning (ICASSP’23 Top 3%)</a>, <a href="https://www.isca-archive.org/interspeech_2023/peng23c_interspeech.html" rel="external nofollow noopener" target="_blank">DPHuBERT (INTERSPEECH’23)</a> </li> <li>Open speech foundation models: <a href="https://arxiv.org/abs/2309.13876" rel="external nofollow noopener" target="_blank">OWSM (ASRU’23)</a>, <a href="https://arxiv.org/abs/2401.16658" rel="external nofollow noopener" target="_blank">OWSM v3.1 (INTERSPEECH’24)</a>, <a href="https://aclanthology.org/2024.acl-long.549/" rel="external nofollow noopener" target="_blank">OWSM-CTC (ACL’24)</a> </li> <li>Speech language models: <a href="https://arxiv.org/abs/2403.12402" rel="external nofollow noopener" target="_blank">SpeechLM analysis</a>, <a href="https://arxiv.org/abs/2403.12408" rel="external nofollow noopener" target="_blank">MSLM-S2ST</a>, <a href="https://arxiv.org/abs/2410.17485" rel="external nofollow noopener" target="_blank">VoiceTextBlender</a>, and more to follow</li> </ul> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%70%65%6E%67%79%66%32%31@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-8581-8674" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=wH2FALMAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/2111014429" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/pyf98" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/yifan-peng" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/pengyf21" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note">The most convenient way to get in touch is via email. </div> </div> <br> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 12vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Dec 04, 2024</th> <td> <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> A co-authored paper received Best Paper Award at SLT 2024 </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 14, 2024</th> <td> <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> A co-authored paper received Best Paper Award at EMNLP 2024 </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 30, 2024</th> <td> <img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> 3 papers are accepted at IEEE SLT 2024 </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 04, 2024</th> <td> <img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> 4 papers (1 first-authored) are accepted at INTERSPEECH 2024 </td> </tr> <tr> <th scope="row" style="width: 20%">May 16, 2024</th> <td> <img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> 1 first-authored paper, OWSM-CTC, is accpeted at ACL 2024 (main) </td> </tr> <tr> <th scope="row" style="width: 20%">May 13, 2024</th> <td> <img class="emoji" title=":man_office_worker:" alt=":man_office_worker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f468-1f4bc.png" height="20" width="20"> Joining NVIDIA NeMo Speech in Santa Clara as AI Research Intern </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 01, 2024</th> <td> <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> We are hosting a special session at INTERSPEECH 2024 - <a href="https://www.wavlab.org/activities/2024/interspeech2024-slm/" rel="external nofollow noopener" target="_blank">Spoken Language Models for Universal Speech Processing</a> (<a href="https://interspeech2024.org/special-sessions-challenges/" rel="external nofollow noopener" target="_blank">Official Site</a>) </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 13, 2023</th> <td> <img class="emoji" title=":spider_web:" alt=":spider_web:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f578.png" height="20" width="20"> Check out the <a href="https://www.wavlab.org/activities/2024/owsm/" rel="external nofollow noopener" target="_blank">webpage</a> for our Open Whisper-style Speech Models (OWSM) </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 13, 2023</th> <td> <img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> 3 papers are accepted at ICASSP 2024 </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 22, 2023</th> <td> <img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> 2 papers (1 first-authored) are accepted at IEEE ASRU 2023 </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 04, 2023</th> <td> <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> 3 papers (2 first-authored) are recognized among the top 3% of all papers accepted at ICASSP 2023 </td> </tr> <tr> <th scope="row" style="width: 20%">May 22, 2023</th> <td> <img class="emoji" title=":man_office_worker:" alt=":man_office_worker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f468-1f4bc.png" height="20" width="20"> Joining Meta AI (FAIR) in Seattle as Research Scientist Intern </td> </tr> <tr> <th scope="row" style="width: 20%">May 17, 2023</th> <td> <img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> 5 papers (2 first-authored) are accepted at INTERSPEECH 2023 </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 17, 2023</th> <td> <img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> 4 research papers (2 first-authored) and 3 co-authored challenge papers are accepted at ICASSP 2023 </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 30, 2022</th> <td> <img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> 2 papers (1 first-authored) are accepted at IEEE SLT 2022 </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 17, 2022</th> <td> <img class="emoji" title=":flight_departure:" alt=":flight_departure:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f6eb.png" height="20" width="20"> Attending ICML 2022 in Baltimore, Maryland, USA </td> </tr> <tr> <th scope="row" style="width: 20%">May 31, 2022</th> <td> <img class="emoji" title=":man_office_worker:" alt=":man_office_worker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f468-1f4bc.png" height="20" width="20"> Joining ASAPP remotely as Speech Recognition Intern </td> </tr> <tr> <th scope="row" style="width: 20%">May 15, 2022</th> <td> <img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> 1 first-authored paper is accepted at ICML 2022 </td> </tr> </table> </div> </div> <br> <br> <h2> <a href="/publications/" style="color: inherit">Select Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#89f2fa"> <div>SLT</div> </abbr> <abbr class="badge rounded w-100">ASR</abbr> </div> <div id="Sudo2024ContextualizedAS" class="col-sm-8"> <div class="title">Contextualized Automatic Speech Recognition with Dynamic Vocabulary</div> <div class="author"> Yui Sudo, Yosuke Fukumoto, Muhammad Shakeel, <em><b>Yifan Peng</b></em>, and Shinji Watanabe </div> <div class="periodical"> <em>In Proceedings of the IEEE Spoken Language Technology Workshop (SLT)</em><span style="color:red"> (Best Paper Award)</span> , Dec 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper Award</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#11d68b"> <div>EMNLP</div> </abbr> <abbr class="badge rounded w-100">Foundation Model</abbr> </div> <div id="Chen2024TowardsRS" class="col-sm-8"> <div class="title">Towards Robust Speech Representation Learning for Thousands of Languages</div> <div class="author"> William Chen, Wangyou Zhang, <em><b>Yifan Peng</b></em>, Xinjian Li, Jinchuan Tian, Jiatong Shi, Xuankai Chang, Soumi Maiti, Karen Livescu, and Shinji Watanabe </div> <div class="periodical"> <em>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</em><span style="color:red"> (Best Paper Award)</span> , Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper Award</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <abbr class="badge rounded w-100">Foundation Model</abbr> </div> <div id="vtblender" class="col-sm-8"> <div class="title">VoiceTextBlender: Augmenting Large Language Models with Speech Capabilities via Single-Stage Joint Speech-Text Supervised Fine-Tuning</div> <div class="author"> <em><b>Yifan Peng<sup>*</sup></b></em>, Krishna C. Puvvada<sup>*</sup>, Zhehuai Chen<sup>*</sup>, Piotr Zelasko, He Huang, Kunal Dhawan, Ke Hu, Shinji Watanabe, Jagadeesh Balam, and Boris Ginsburg </div> <div class="periodical"> <em>ArXiv</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2410.17485" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2410.17485" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#89f2fa"> <div>INTERSPEECH</div> </abbr> <abbr class="badge rounded w-100">Foundation Model</abbr> </div> <div id="Peng2024OWSMVB" class="col-sm-8"> <div class="title">OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer</div> <div class="author"> <em><b>Yifan Peng</b></em>, Jinchuan Tian, William Chen, Siddhant Arora, Brian Yan, Yui Sudo, Muhammad Shakeel, Kwanghee Choi, Jiatong Shi, Xuankai Chang, Jee-weon Jung, and Shinji Watanabe </div> <div class="periodical"> <em>In Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2401.16658" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2401.16658" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/espnet/espnet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/owsmv31-is24.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://www.wavlab.org/activities/2024/owsm/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=wH2FALMAAAAJ&amp;citation_for_view=wH2FALMAAAAJ:Zph67rFs4hoC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-27-4285F4?logo=googlescholar&amp;labelColor=beige" alt="27 Google Scholar citations"> </a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#11d68b"> <div>ACL</div> </abbr> <abbr class="badge rounded w-100">Foundation Model</abbr> </div> <div id="peng-etal-2024-owsm" class="col-sm-8"> <div class="title">OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification</div> <div class="author"> <em><b>Yifan Peng</b></em>, Yui Sudo, Muhammad Shakeel, and Shinji Watanabe </div> <div class="periodical"> <em>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</em>, Aug 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.12654" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://aclanthology.org/2024.acl-long.549.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/pyf98/espnet/tree/owsm-ctc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/owsm-ctc-acl24.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://huggingface.co/pyf98/owsm_ctc_v3.1_1B" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=wH2FALMAAAAJ&amp;citation_for_view=wH2FALMAAAAJ:_kc_bZDykSQC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-12-4285F4?logo=googlescholar&amp;labelColor=beige" alt="12 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>There has been an increasing interest in large speech models that can perform multiple tasks in a single model. Such models usually adopt an encoder-decoder or decoder-only architecture due to their popularity and good performance in many domains. However, autoregressive models can be slower during inference compared to non-autoregressive models and also have potential risks of hallucination. Though prior studies observed promising results of non-autoregressive models for certain tasks at small scales, it remains unclear if they can be scaled to speech-to-text generation in diverse languages and tasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we propose OWSM-CTC, a novel encoder-only speech foundation model based on Connectionist Temporal Classification (CTC). It is trained on 180k hours of public audio data for multilingual automatic speech recognition (ASR), speech translation (ST), and language identification (LID). Compared to encoder-decoder OWSM, our OWSM-CTC achieves competitive results on ASR and up to 24% relative improvement on ST, while it is more robust and 3 to 4 times faster for inference. OWSM-CTC also improves the long-form ASR result with 20x speed-up.We will publicly release our code, pre-trained model, and training logs to promote open science in speech foundation models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#89f2fa"> <div>ICASSP</div> </abbr> <abbr class="badge rounded w-100">Foundation Model</abbr> </div> <div id="Maiti2023VoxtLMUD" class="col-sm-8"> <div class="title">VoxtLM: Unified Decoder-Only Models for Consolidating Speech Recognition, Synthesis and Speech, Text Continuation Tasks</div> <div class="author"> Soumi Maiti, <em><b>Yifan Peng</b></em>, Shukjae Choi, Jee-weon Jung, Xuankai Chang, and Shinji Watanabe </div> <div class="periodical"> <em>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, Apr 2024 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=wH2FALMAAAAJ&amp;citation_for_view=wH2FALMAAAAJ:5nxA0vEk-isC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-48-4285F4?logo=googlescholar&amp;labelColor=beige" alt="48 Google Scholar citations"> </a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <abbr class="badge rounded w-100">Foundation Model</abbr> </div> <div id="Peng2024MSLMS2STAM" class="col-sm-8"> <div class="title">MSLM-S2ST: A Multitask Speech Language Model for Textless Speech-to-Speech Translation with Speaker Style Preservation</div> <div class="author"> <em><b>Yifan Peng</b></em>, Ilia Kulikov, Yilin Yang, Sravya Popuri, Hui Lu, Changhan Wang, and Hongyu Gong </div> <div class="periodical"> <em>ArXiv</em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2403.12408" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2403.12408" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <abbr class="badge rounded w-100">Foundation Model</abbr> </div> <div id="Peng2024AnES" class="col-sm-8"> <div class="title">An Empirical Study of Speech Language Models for Prompt-Conditioned Speech Synthesis</div> <div class="author"> <em><b>Yifan Peng</b></em>, Ilia Kulikov, Yilin Yang, Sravya Popuri, Hui Lu, Changhan Wang, and Hongyu Gong </div> <div class="periodical"> <em>ArXiv</em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2403.12402" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2403.12402" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#89f2fa"> <div>ASRU</div> </abbr> <abbr class="badge rounded w-100">Foundation Model</abbr> </div> <div id="Peng2023ReproducingWT" class="col-sm-8"> <div class="title">Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data</div> <div class="author"> <em><b>Yifan Peng</b></em>, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou Zhang, Yui Sudo, Muhammad Shakeel, Jee-weon Jung, Soumi Maiti, and Shinji Watanabe </div> <div class="periodical"> <em>In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2309.13876" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2309.13876" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.wavlab.org/activities/2024/owsm/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=wH2FALMAAAAJ&amp;citation_for_view=wH2FALMAAAAJ:8k81kl-MbHgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-38-4285F4?logo=googlescholar&amp;labelColor=beige" alt="38 Google Scholar citations"> </a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#89f2fa"> <div>INTERSPEECH</div> </abbr> <abbr class="badge rounded w-100">Efficient Model</abbr> </div> <div id="Peng2023DPHuBERTJD" class="col-sm-8"> <div class="title">DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models</div> <div class="author"> <em><b>Yifan Peng</b></em>, Yui Sudo, Muhammad Shakeel, and Shinji Watanabe </div> <div class="periodical"> <em>In Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)</em>, Aug 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2305.17651" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2305.17651" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/pyf98/DPHuBERT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=wH2FALMAAAAJ&amp;citation_for_view=wH2FALMAAAAJ:LkGwnXOMwfcC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-39-4285F4?logo=googlescholar&amp;labelColor=beige" alt="39 Google Scholar citations"> </a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#89f2fa"> <div>INTERSPEECH</div> </abbr> <abbr class="badge rounded w-100">Architecture</abbr> </div> <div id="Peng2023ACS" class="col-sm-8"> <div class="title">A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks</div> <div class="author"> <em><b>Yifan Peng</b></em>, Kwangyoun Kim, Felix Wu, Brian Yan, Siddhant Arora, William Chen, Jiyang Tang, Suwon Shon, Prashant Sridhar, and Shinji Watanabe </div> <div class="periodical"> <em>In Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)</em>, Aug 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2305.11073" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2305.11073" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/espnet/espnet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=wH2FALMAAAAJ&amp;citation_for_view=wH2FALMAAAAJ:_FxGoFyzp5QC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-16-4285F4?logo=googlescholar&amp;labelColor=beige" alt="16 Google Scholar citations"> </a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#89f2fa"> <div>ICASSP</div> </abbr> <abbr class="badge rounded w-100">Efficient Model</abbr> </div> <div id="Peng2023I3DTA" class="col-sm-8"> <div class="title">I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition</div> <div class="author"> <em><b>Yifan Peng</b></em>, Jaesong Lee, and Shinji Watanabe </div> <div class="periodical"> <em>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em><span style="color:red"> (Top 3% of all papers accepted)</span> , Jun 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a href="http://arxiv.org/abs/2303.07624" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2303.07624" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=wH2FALMAAAAJ&amp;citation_for_view=wH2FALMAAAAJ:W7OEmFMy1HYC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-9-4285F4?logo=googlescholar&amp;labelColor=beige" alt="9 Google Scholar citations"> </a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Recognized as one of the top 3% of all papers accepted at the International Conference on Acoustics Speech and Signal Processing (ICASSP) 2023</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#89f2fa"> <div>ICASSP</div> </abbr> <abbr class="badge rounded w-100">Efficient Model</abbr> </div> <div id="Peng2023StructuredPO" class="col-sm-8"> <div class="title">Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding</div> <div class="author"> <em><b>Yifan Peng</b></em>, Kwangyoun Kim, Felix Wu, Prashant Sridhar, and Shinji Watanabe </div> <div class="periodical"> <em>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em><span style="color:red"> (Top 3% of all papers accepted)</span> , Jun 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a href="http://arxiv.org/abs/https://arxiv.org/pdf/2302.14132" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/2302.14132" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=wH2FALMAAAAJ&amp;citation_for_view=wH2FALMAAAAJ:Y0pCki6q_DkC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-33-4285F4?logo=googlescholar&amp;labelColor=beige" alt="33 Google Scholar citations"> </a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Recognized as one of the top 3% of all papers accepted at the International Conference on Acoustics Speech and Signal Processing (ICASSP) 2023</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#89f2fa"> <div>SLT</div> </abbr> <abbr class="badge rounded w-100">SLU</abbr> </div> <div id="Peng2022ASO" class="col-sm-8"> <div class="title">A Study on the Integration of Pre-Trained SSL, ASR, LM and SLU Models for Spoken Language Understanding</div> <div class="author"> <em><b>Yifan Peng<sup>*</sup></b></em>, Siddhant Arora<sup>*</sup>, Yosuke Higuchi, Yushi Ueda, Sujay S. Kumar, Karthik Ganesan, Siddharth Dalmia, Xuankai Chang, and Shinji Watanabe <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* Equal contribution"> </i> </div> <div class="periodical"> <em>In Proceedings of the 2022 IEEE Spoken Language Technology Workshop (SLT)</em>, Jan 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2211.05869" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2211.05869" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=wH2FALMAAAAJ&amp;citation_for_view=wH2FALMAAAAJ:IjCSPb-OGe4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-24-4285F4?logo=googlescholar&amp;labelColor=beige" alt="24 Google Scholar citations"> </a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#89f2fa"> <div>SLT</div> </abbr> <abbr class="badge rounded w-100">Architecture</abbr> </div> <div id="Kim2022EBranchformerBW" class="col-sm-8"> <div class="title">E-Branchformer: Branchformer with Enhanced Merging for Speech Recognition</div> <div class="author"> Kwangyoun Kim, Felix Wu, <em><b>Yifan Peng</b></em>, Jing Pan, Prashant Sridhar, Kyu J. Han, and Shinji Watanabe </div> <div class="periodical"> <em>In Proceedings of the 2022 IEEE Spoken Language Technology Workshop (SLT)</em>, Jan 2023 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=wH2FALMAAAAJ&amp;citation_for_view=wH2FALMAAAAJ:UeHWp8X0CEIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-100-4285F4?logo=googlescholar&amp;labelColor=beige" alt="100 Google Scholar citations"> </a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#f79292"> <div>ICML</div> </abbr> <abbr class="badge rounded w-100">Architecture</abbr> </div> <div id="pmlr-v162-peng22a" class="col-sm-8"> <div class="title">Branchformer: Parallel MLP-Attention Architectures to Capture Local and Global Context for Speech Recognition and Understanding</div> <div class="author"> <em><b>Yifan Peng</b></em>, Siddharth Dalmia, Ian Lane, and Shinji Watanabe </div> <div class="periodical"> <em>In Proceedings of the International Conference on Machine Learning (ICML)</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v162/peng22a/peng22a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://slideslive.com/38983369/branchformer-parallel-mlpattention-architectures-to-capture-local-and-global-context-for-speech-recognition-and-understanding" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/espnet/espnet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://icml.cc/media/PosterPDFs/ICML%202022/2adcfc3929e7c03fac3100d3ad51da26.png" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> <a href="https://icml.cc/media/icml-2022/Slides/18226.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=wH2FALMAAAAJ&amp;citation_for_view=wH2FALMAAAAJ:2osOgNQ5qMEC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-160-4285F4?logo=googlescholar&amp;labelColor=beige" alt="160 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Conformer has proven to be effective in many speech processing tasks. It combines the benefits of extracting local dependencies using convolutions and global dependencies using self-attention. Inspired by this, we propose a more flexible, interpretable and customizable encoder alternative, Branchformer, with parallel branches for modeling various ranged dependencies in end-to-end speech processing. In each encoder layer, one branch employs self-attention or its variant to capture long-range dependencies, while the other branch utilizes an MLP module with convolutional gating (cgMLP) to extract local relationships. We conduct experiments on several speech recognition and spoken language understanding benchmarks. Results show that our model outperforms both Transformer and cgMLP. It also matches with or outperforms state-of-the-art results achieved by Conformer. Furthermore, we show various strategies to reduce computation thanks to the two-branch architecture, including the ability to have variable inference complexity in a single trained model. The weights learned for merging branches indicate how local and global dependencies are utilized in different layers, which benefits model designing.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">SPIE</abbr> <abbr class="badge rounded w-100">Others</abbr> </div> <div id="Peng2020MicrocalcificationLA" class="col-sm-8"> <div class="title">Microcalcification localization and cluster detection using unsupervised convolutional autoencoders and structural similarity index</div> <div class="author"> <em><b>Yifan Peng</b></em>, Rui Hou, Yinhao Ren, Lars J. Grimm, Jeffrey R. Marks, E. Shelley Hwang, and Joseph Y. Lo </div> <div class="periodical"> <em>In Proceedings of the SPIE Medical Imaging 2020: Computer-Aided Diagnosis</em><span style="color:red"> (Robert F. Wagner Best Student Paper Award Finalist)</span> , May 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11314/1131403/Microcalcification-localization-and-cluster-detection-using-unsupervised-convolutional-autoencoders-and/10.1117/12.2551263.short#_=_" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Robert F. Wagner Best Student Paper Award Finalist at SPIE Medical Imaging 2020</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Yifan Peng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: December 29, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"Publications in reversed chronological order.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-scroll-1-first-authored-paper-is-accepted-at-icml-2022",title:'<img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> 1 first-authored paper is accepted at ICML 2022',description:"",section:"News"},{id:"news-man-office-worker-joining-asapp-remotely-as-speech-recognition-intern",title:'<img class="emoji" title=":man_office_worker:" alt=":man_office_worker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f468-1f4bc.png" height="20" width="20"> Joining ASAPP remotely as Speech Recognition Intern',description:"",section:"News"},{id:"news-flight-departure-attending-icml-2022-in-baltimore-maryland-usa",title:'<img class="emoji" title=":flight_departure:" alt=":flight_departure:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f6eb.png" height="20" width="20"> Attending ICML 2022 in Baltimore, Maryland, USA',description:"",section:"News"},{id:"news-scroll-2-papers-1-first-authored-are-accepted-at-ieee-slt-2022",title:'<img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> 2 papers (1 first-authored) are accepted at IEEE SLT 2022',description:"",section:"News"},{id:"news-scroll-4-research-papers-2-first-authored-and-3-co-authored-challenge-papers-are-accepted-at-icassp-2023",title:'<img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> 4 research papers (2 first-authored) and 3 co-authored challenge papers are accepted...',description:"",section:"News"},{id:"news-scroll-5-papers-2-first-authored-are-accepted-at-interspeech-2023",title:'<img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> 5 papers (2 first-authored) are accepted at INTERSPEECH 2023',description:"",section:"News"},{id:"news-man-office-worker-joining-meta-ai-fair-in-seattle-as-research-scientist-intern",title:'<img class="emoji" title=":man_office_worker:" alt=":man_office_worker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f468-1f4bc.png" height="20" width="20"> Joining Meta AI (FAIR) in Seattle as Research Scientist Intern',description:"",section:"News"},{id:"news-trophy-3-papers-2-first-authored-are-recognized-among-the-top-3-of-all-papers-accepted-at-icassp-2023",title:'<img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> 3 papers (2 first-authored) are recognized among the top 3% of all...',description:"",section:"News"},{id:"news-scroll-2-papers-1-first-authored-are-accepted-at-ieee-asru-2023",title:'<img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> 2 papers (1 first-authored) are accepted at IEEE ASRU 2023',description:"",section:"News"},{id:"news-scroll-3-papers-are-accepted-at-icassp-2024",title:'<img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> 3 papers are accepted at ICASSP 2024',description:"",section:"News"},{id:"news-spider-web-check-out-the-webpage-for-our-open-whisper-style-speech-models-owsm",title:'<img class="emoji" title=":spider_web:" alt=":spider_web:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f578.png" height="20" width="20"> Check out the webpage for our Open Whisper-style Speech Models (OWSM)',description:"",section:"News"},{id:"news-sparkles-we-are-hosting-a-special-session-at-interspeech-2024-spoken-language-models-for-universal-speech-processing-official-site",title:'<img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> We are hosting a special session at INTERSPEECH 2024 - Spoken Language...',description:"",section:"News"},{id:"news-man-office-worker-joining-nvidia-nemo-speech-in-santa-clara-as-ai-research-intern",title:'<img class="emoji" title=":man_office_worker:" alt=":man_office_worker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f468-1f4bc.png" height="20" width="20"> Joining NVIDIA NeMo Speech in Santa Clara as AI Research Intern',description:"",section:"News"},{id:"news-scroll-1-first-authored-paper-owsm-ctc-is-accpeted-at-acl-2024-main",title:'<img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> 1 first-authored paper, OWSM-CTC, is accpeted at ACL 2024 (main)',description:"",section:"News"},{id:"news-scroll-4-papers-1-first-authored-are-accepted-at-interspeech-2024",title:'<img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> 4 papers (1 first-authored) are accepted at INTERSPEECH 2024',description:"",section:"News"},{id:"news-scroll-3-papers-are-accepted-at-ieee-slt-2024",title:'<img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> 3 papers are accepted at IEEE SLT 2024',description:"",section:"News"},{id:"news-trophy-a-co-authored-paper-received-best-paper-award-at-emnlp-2024",title:'<img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> A co-authored paper received Best Paper Award at EMNLP 2024',description:"",section:"News"},{id:"news-trophy-a-co-authored-paper-received-best-paper-award-at-slt-2024",title:'<img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> A co-authored paper received Best Paper Award at SLT 2024',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%70%65%6E%67%79%66%32%31@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0002-8581-8674","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=wH2FALMAAAAJ","_blank")}},{id:"socials-semantic-scholar",title:"Semantic Scholar",section:"Socials",handler:()=>{window.open("https://www.semanticscholar.org/author/2111014429","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/pyf98","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/yifan-peng","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/pengyf21","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>